{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preprocessing the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before begining with the model and training, the dataset first needs to be preprocessed\n",
    "# This is a very important step in all of machine learning\n",
    "\n",
    "# The MNIST dataset is, in general, highly processed already - after all its 28x28 grayscale images of clearly visible digits\n",
    "# Thus, the preprocessing will be limited to scaling the pixel values, shuffling the data and creating a validation set\n",
    "\n",
    "# NOTE: When finally deploying a model in practice, it might be a good idea to include the prerpocessing as initial layers\n",
    "# In that way, the users could just plug the data (images) directly, instead of being required to resize/rescale it before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some constants/hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 70_000 # for reshuffling\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# When 'with_info' is set to True, tfds.load() returns two variables: \n",
    "# - the dataset (including the train and test sets) \n",
    "# - meta info regarding the dataset itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the images\n",
    "\n",
    "Since each image is a gray-scale image with each pixel ranging from 0 to 255, it would be nice to rescale all of them to values ranging from 0 to 1 by simply dividing them by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_images(image, label):\n",
    "    image = tf.cast(image, tf.float32) # Make sure all rescaled images will be of type float32\n",
    "    image /= 255.0 # Achieve the scaling by dividing each image by 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trained_and_validation_data = mnist_train.map(rescale_images) # Maps the old data to new scale\n",
    "scaled_test_data = mnist_test.map(rescale_images) # Maps the old data to new scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trained_and_validation_data = scaled_trained_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "scaled_test_data = scaled_test_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Splitting\n",
    "Splitting the now Shuffled scaled_trained_and_validation_data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using train_test_split() from sklearn to split my dataset\n",
    "\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_trained_data, scaled_validation_data = train_test_split(scaled_trained_and_validation_data,\n",
    "#                                                               test_size=0.1,\n",
    "#                                                               random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using train_test_split's  **test_size=0.1** does not return an integer value as size of the scaled_validation_data above so I will have to do the splitting manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the validation set\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the test set\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training + validation\n",
    "train_data = scaled_trained_and_validation_data.skip(num_validation_samples)\n",
    "validation_data = scaled_trained_and_validation_data.take(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the datasets\n",
    "\n",
    "For proper functioning of the model, the batch size for the validation and test sets needs to be one big size that can take all the specific datasets into one batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "scaled_test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the numpy arrays from the validation data for the calculation of the Confusion Matrix\n",
    "for images, labels in validation_data:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of the model/architecture of the CNN to be implemented\n",
    "\n",
    "CONV -> MAXPOOL -> CONV -> MAXPOOL -> FLATTEN -> DENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation=\"relu\", input_shape=(28,28,1)), # 50 kernels of size 5X5\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), # input_shape is needed only when the layer is the first one\n",
    "    # (2,2) is the default pool size so it could have just used MaxPooling2D() with no explicit arguments\n",
    "    tf.keras.layers.Conv2D(50, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10) # Number of desired output layer\n",
    "])\n",
    "\n",
    "# NB* The activation function was not included in the last layer as it is impossible to provide \n",
    "# Numerically stable loss calculations for all Models\n",
    "# The softmax can be incorporated into the loss function itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "___________________________________________________________________________\n",
      "Layer (type)                     Output Shape                  Param #     \n",
      "===========================================================================\n",
      "conv2d (Conv2D)                  (None, 24, 24, 50)            1300        \n",
      "___________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)     (None, 12, 12, 50)            0           \n",
      "___________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 10, 10, 50)            22550       \n",
      "___________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 5, 5, 50)              0           \n",
      "___________________________________________________________________________\n",
      "flatten (Flatten)                (None, 1250)                  0           \n",
      "___________________________________________________________________________\n",
      "dense (Dense)                    (None, 10)                    12510       \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length = 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, the model needs to output the probabilities for each class, \n",
    "# this can be achieved with a softmax activation in the last dense layer\n",
    "\n",
    "# However, when using the softmax activation, the loss can rarely be unstable\n",
    "\n",
    "# Thus, instead of incorporating the softmax into the model itself,\n",
    "# the loss calculation can be use which automatically corrects for the missing softmax\n",
    "\n",
    "# That is the reason for 'from_logits=True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss_func, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"Logs\\\\fit\\\\\" + \"run-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Confusion Matrix Plot to an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    \n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "    plt.close(figure)\n",
    "    \n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file writer variable for logging purposes\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(images_val)\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the callbacks\n",
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', # monitor the validation loss function\n",
    "    mode = 'auto',    \n",
    "    min_delta = 0,\n",
    "    patience = 2, # Stop when val_loss starts increasing after two epochs\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log only Tensor Board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging the training process data to use later in tensorboard\n",
    "# log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 - 125s - loss: 0.0717 - accuracy: 0.9786 - val_loss: 0.0540 - val_accuracy: 0.9810\n",
      "Epoch 2/20\n",
      "422/422 - 120s - loss: 0.0547 - accuracy: 0.9834 - val_loss: 0.0401 - val_accuracy: 0.9875\n",
      "Epoch 3/20\n",
      "422/422 - 135s - loss: 0.0450 - accuracy: 0.9864 - val_loss: 0.0428 - val_accuracy: 0.9870\n",
      "Epoch 4/20\n",
      "422/422 - 131s - loss: 0.0383 - accuracy: 0.9884 - val_loss: 0.0285 - val_accuracy: 0.9915\n",
      "Epoch 5/20\n",
      "422/422 - 129s - loss: 0.0329 - accuracy: 0.9896 - val_loss: 0.0218 - val_accuracy: 0.9932\n",
      "Epoch 6/20\n",
      "422/422 - 129s - loss: 0.0284 - accuracy: 0.9912 - val_loss: 0.0266 - val_accuracy: 0.9913\n",
      "Epoch 7/20\n",
      "422/422 - 123s - loss: 0.0255 - accuracy: 0.9924 - val_loss: 0.0202 - val_accuracy: 0.9938\n",
      "Epoch 8/20\n",
      "422/422 - 124s - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0188 - val_accuracy: 0.9933\n",
      "Epoch 9/20\n",
      "422/422 - 126s - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.0137 - val_accuracy: 0.9965\n",
      "Epoch 10/20\n",
      "422/422 - 131s - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.0156 - val_accuracy: 0.9948\n",
      "Epoch 11/20\n",
      "422/422 - 138s - loss: 0.0155 - accuracy: 0.9952 - val_loss: 0.0169 - val_accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cbfa6dae50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs = NUM_EPOCHS, \n",
    "    callbacks = [tensorboard_callback, cm_callback, early_stopping], # early_stopping should always be the last clbc\n",
    "    validation_data = validation_data,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step - loss: 0.0335 - accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(scaled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0335. Test accuracy: 98.93%\n"
     ]
    }
   ],
   "source": [
    "# Printing the test results\n",
    "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting images and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the scaled_test_data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in scaled_test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "# Reshape the images into 28x28 form, suitable for matplotlib (original dimensions: 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The image to be displayed and tested\n",
    "\n",
    "**i = 502**  Change this value to get predictions of different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB7CAYAAACy7jQ7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE2ElEQVR4nO2dSyhtbxjG/9s1JKHITJQSA8mQMHEbuEwMGIhSUkgumSAZEGZmZkwwcilFYaDEQG4xMyAjl9yTsuWMzuB917+1Wtbee63jeX6zX2d/e737PH3e/a21vrV939/f3/8ROMLcLoC4A4MHhcGDwuBBYfCgMHhQIsz+0efzhaoOEgTMVuqc8aAweFAYPCgMHhQGDwqDB4XBg8LgQWHwoDB4UBg8KAweFAYPCoMHhcGDYno93uvExsYKb21tFV5WVia8qqpK+NTUlHC/3y/85eVF+NjY2I/q9CKc8aAweFB8Zjtp3L71KjExUXhTU5Pwvr4+4WlpaY6Opz/vzc2N8IyMDOHv7++OjhdseOsVMcDgQWHwoHiqx0dFRQlfWVkRXl5ebuv99HLs9vZWeGZmpvCwMDkP9H/N7u6u8O7ubuEHBwe26gs27PHEAIMHhcGD4qke39/fL3x8fNzW+L29PeFdXV3CdQ+enJwU3tvbK9zqYSH6O4g+r3BxcWE6PtiwxxMDDB4UBg+Kqz1er9uXlpaEV1ZWmo5/fHwU3tDQIHxjY8NWPaOjo8JPTk6ET09PC09NTTWtp7GxUfj6+rqtepzCHk8MMHhQGDworvb4wsJC4Ts7O7bG656+sLDguCYzSktLhW9tbZm+3u2ezx5PDDB4UBg8KCHt8ZGRkcL1ue6KigrT8UdHR8KLi4uFv729OajOmujoaOETExPCOzo6TMfv7+8L7+zsFB7o6/ns8cQAgweFwYMS0h4fHx8v/Pn52dZ4vQ6en593XJMT8vLyhB8eHtoa//r6KlzfA3h/f/+juv7CHk8MMHhQGDwont4mfXd3J1yv493m/PxcuL7PXt9DqPf26e88s7Ozwmtra4V/fn7+pMz/hTMeFAYPCoMHJaTr+JGREeGDg4Omrz8+Phaen58f0HqCTW5urvDNzU3hKSkppuP1PYDDw8O2js91PDHA4EEJ6XKupKREuFUrCeTyxQ3Ozs6E68uwi4uLpuMjIoIXD2c8KAweFAYPSkh7vH6USFFRkenr9Tbm34bVNuxgwhkPCoMHhcGD4unLsklJSW6X8GvhjAeFwYPC4EHxdI9va2sTPjMz41Ilvw/OeFAYPCgMHpSQ9nh9/XlgYMD09XFxccL1uv7h4SEwhQUJXW9LS4ut8dfX14EsR8AZDwqDB4XBg/JPbZPWW5aGhoaE60eihpqCggLh+vZoq59Wubq6Ep6dnS384+PDVj28vZoYYPCgMHhQQtrjw8PDhff09AjX5+bT09NN308/SmRtbU348vKy8NXVVeFWPVM/6iQrK0t4XV2dcP1r1fo7jRX6eKenp7bGa9jjiQEGDwqDB8VTPz/W3NwsvKamRnh1dbWj97+8vBT+9fUlXH/e5ORk4QkJCQE9fn19vXC9Ldzv9zs6Hns8McDgQWHwoHiqx2v0Y87b29uFWz1OzC7689rd26bPE2xvbwufm5sT/vT0ZOv97cIeTwwweFAYPCie7vFWxMTECM/JyRGuz6XrZ9Doe/r0utlqf76+/h/odbhT2OOJAQYPyj/9p56Ywz/1xACDB4XBg8LgQWHwoDB4UBg8KAweFAYPCoMHhcGDwuBBYfCgMHhQGDwopo87c/OnM0hw4YwHhcGDwuBBYfCgMHhQGDwofwBSvIBMLaQeowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# i = 502 # Change this value to get predictions of different images\n",
    "\n",
    "i = 34\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel(\"Image\", size=19)\n",
    "plt.ylabel(\"Probability of prediction\", size=19)\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualizing in Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1608), started 23:48:31 ago. (Use '!kill 1608' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-65f4467420d9873d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-65f4467420d9873d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/fit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rerun tensorboard after the first run if there are any problems, open cmd and use the following commands\n",
    "\n",
    ".../>taskkill /im tensorboard.exe /f\n",
    "\n",
    ".../>del /q %TMP%\\.tensorboard-info\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
