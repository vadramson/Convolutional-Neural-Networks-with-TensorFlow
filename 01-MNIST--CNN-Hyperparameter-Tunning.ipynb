{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preprocessing the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before begining with the model and training, the dataset first needs to be preprocessed\n",
    "# This is a very important step in all of machine learning\n",
    "\n",
    "# The MNIST dataset is, in general, highly processed already - after all its 28x28 grayscale images of clearly visible digits\n",
    "# Thus, the preprocessing will be limited to scaling the pixel values, shuffling the data and creating a validation set\n",
    "\n",
    "# NOTE: When finally deploying a model in practice, it might be a good idea to include the prerpocessing as initial layers\n",
    "# In that way, the users could just plug the data (images) directly, instead of being required to resize/rescale it before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some constants/hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 70_000 # for reshuffling\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# When 'with_info' is set to True, tfds.load() returns two variables: \n",
    "# - the dataset (including the train and test sets) \n",
    "# - meta info regarding the dataset itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the images\n",
    "\n",
    "Since each image is a gray-scale image with each pixel ranging from 0 to 255, it would be nice to rescale all of them to values ranging from 0 to 1 by simply dividing them by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_images(image, label):\n",
    "    image = tf.cast(image, tf.float32) # Make sure all rescaled images will be of type float32\n",
    "    image /= 255.0 # Achieve the scaling by dividing each image by 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trained_and_validation_data = mnist_train.map(rescale_images) # Maps the old data to new scale\n",
    "scaled_test_data = mnist_test.map(rescale_images) # Maps the old data to new scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_trained_and_validation_data = scaled_trained_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "scaled_test_data = scaled_test_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Splitting\n",
    "Splitting the now Shuffled scaled_trained_and_validation_data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using train_test_split() from sklearn to split my dataset\n",
    "\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_trained_data, scaled_validation_data = train_test_split(scaled_trained_and_validation_data,\n",
    "#                                                               test_size=0.1,\n",
    "#                                                               random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using train_test_split's  **test_size=0.1** does not return an integer value as size of the scaled_validation_data above so I will have to do the splitting manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the validation set\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the test set\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training + validation\n",
    "train_data = scaled_trained_and_validation_data.skip(num_validation_samples)\n",
    "validation_data = scaled_trained_and_validation_data.take(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching the datasets\n",
    "\n",
    "For proper functioning of the model, the batch size for the validation and test sets needs to be one big size that can take all the specific datasets into one batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "scaled_test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the numpy arrays from the validation data for the calculation of the Confusion Matrix\n",
    "for images, labels in validation_data:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters to be used in tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hypermatarest we would test and their range\n",
    "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3,5,7]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# Logging setup info\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_FILTER_SIZE, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cerate functions for training and logging purposes of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of the model/architecture of the CNN to be implemented\n",
    "\n",
    "CONV -> MAXPOOL -> CONV -> MAXPOOL -> FLATTEN -> DENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping the model and training in a functions\n",
    "def train_test_model(hparams):\n",
    "    \n",
    "    # Outlining the model/architecture of our CNN\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(50, hparams[HP_FILTER_SIZE], activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(50, hparams[HP_FILTER_SIZE], activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    # Defining the loss function\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compiling the model with parameter value for the optimizer\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    # Defining early stopping to prevent overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        mode = 'auto',\n",
    "        min_delta = 0,\n",
    "        patience = 2,\n",
    "        verbose = 0, \n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(\n",
    "        train_data, \n",
    "        epochs = NUM_EPOCHS,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data = validation_data,\n",
    "        verbose = 2\n",
    "    )\n",
    "    \n",
    "    _, accuracy = model.evaluate(scaled_test_data)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to log the resuls\n",
    "def run(log_dir, hparams):\n",
    "    \n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with the different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'filter_size': 3, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 - 110s - loss: 0.2911 - accuracy: 0.9182 - val_loss: 0.0971 - val_accuracy: 0.9712\n",
      "Epoch 2/20\n",
      "422/422 - 107s - loss: 0.0794 - accuracy: 0.9763 - val_loss: 0.0672 - val_accuracy: 0.9788\n",
      "Epoch 3/20\n",
      "422/422 - 111s - loss: 0.0565 - accuracy: 0.9825 - val_loss: 0.0517 - val_accuracy: 0.9842\n",
      "Epoch 4/20\n",
      "422/422 - 116s - loss: 0.0475 - accuracy: 0.9852 - val_loss: 0.0479 - val_accuracy: 0.9860\n",
      "Epoch 5/20\n",
      "422/422 - 114s - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.0238 - val_accuracy: 0.9928\n",
      "Epoch 6/20\n",
      "422/422 - 118s - loss: 0.0352 - accuracy: 0.9894 - val_loss: 0.0244 - val_accuracy: 0.9937\n",
      "Epoch 7/20\n",
      "422/422 - 113s - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.0291 - val_accuracy: 0.9903\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0406 - accuracy: 0.9872\n",
      "--- Starting trial: run-1\n",
      "{'filter_size': 3, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 - 117s - loss: 1.3576 - accuracy: 0.6620 - val_loss: 0.4842 - val_accuracy: 0.8657\n",
      "Epoch 2/20\n",
      "422/422 - 116s - loss: 0.3762 - accuracy: 0.8909 - val_loss: 0.3066 - val_accuracy: 0.9120\n",
      "Epoch 3/20\n",
      "422/422 - 110s - loss: 0.2860 - accuracy: 0.9162 - val_loss: 0.2599 - val_accuracy: 0.9240\n",
      "Epoch 4/20\n",
      "422/422 - 118s - loss: 0.2355 - accuracy: 0.9315 - val_loss: 0.2189 - val_accuracy: 0.9323\n",
      "Epoch 5/20\n",
      "422/422 - 115s - loss: 0.1991 - accuracy: 0.9416 - val_loss: 0.1959 - val_accuracy: 0.9435\n",
      "Epoch 6/20\n",
      "422/422 - 109s - loss: 0.1764 - accuracy: 0.9481 - val_loss: 0.1676 - val_accuracy: 0.9485\n",
      "Epoch 7/20\n",
      "422/422 - 121s - loss: 0.1565 - accuracy: 0.9543 - val_loss: 0.1468 - val_accuracy: 0.9567\n",
      "Epoch 8/20\n",
      "422/422 - 118s - loss: 0.1406 - accuracy: 0.9593 - val_loss: 0.1419 - val_accuracy: 0.9577\n",
      "Epoch 9/20\n",
      "422/422 - 117s - loss: 0.1285 - accuracy: 0.9624 - val_loss: 0.1311 - val_accuracy: 0.9607\n",
      "Epoch 10/20\n",
      "422/422 - 119s - loss: 0.1213 - accuracy: 0.9650 - val_loss: 0.1198 - val_accuracy: 0.9627\n",
      "Epoch 11/20\n",
      "422/422 - 101s - loss: 0.1143 - accuracy: 0.9662 - val_loss: 0.1241 - val_accuracy: 0.9652\n",
      "Epoch 12/20\n",
      "422/422 - 114s - loss: 0.1057 - accuracy: 0.9691 - val_loss: 0.0951 - val_accuracy: 0.9713\n",
      "Epoch 13/20\n",
      "422/422 - 116s - loss: 0.1010 - accuracy: 0.9701 - val_loss: 0.0923 - val_accuracy: 0.9753\n",
      "Epoch 14/20\n",
      "422/422 - 119s - loss: 0.0968 - accuracy: 0.9709 - val_loss: 0.0843 - val_accuracy: 0.9767\n",
      "Epoch 15/20\n",
      "422/422 - 116s - loss: 0.0935 - accuracy: 0.9727 - val_loss: 0.0878 - val_accuracy: 0.9773\n",
      "Epoch 16/20\n",
      "422/422 - 111s - loss: 0.0899 - accuracy: 0.9733 - val_loss: 0.0853 - val_accuracy: 0.9753\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0854 - accuracy: 0.9751\n",
      "--- Starting trial: run-2\n",
      "{'filter_size': 5, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 - 127s - loss: 0.2484 - accuracy: 0.9308 - val_loss: 0.0747 - val_accuracy: 0.9798\n",
      "Epoch 2/20\n",
      "422/422 - 128s - loss: 0.0662 - accuracy: 0.9803 - val_loss: 0.0601 - val_accuracy: 0.9847\n",
      "Epoch 3/20\n",
      "422/422 - 124s - loss: 0.0468 - accuracy: 0.9853 - val_loss: 0.0376 - val_accuracy: 0.9878\n",
      "Epoch 4/20\n",
      "422/422 - 132s - loss: 0.0368 - accuracy: 0.9892 - val_loss: 0.0276 - val_accuracy: 0.9917\n",
      "Epoch 5/20\n",
      "422/422 - 120s - loss: 0.0304 - accuracy: 0.9904 - val_loss: 0.0241 - val_accuracy: 0.9932\n",
      "Epoch 6/20\n",
      "422/422 - 133s - loss: 0.0243 - accuracy: 0.9928 - val_loss: 0.0189 - val_accuracy: 0.9940\n",
      "Epoch 7/20\n",
      "422/422 - 121s - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.0209 - val_accuracy: 0.9937\n",
      "Epoch 8/20\n",
      "422/422 - 128s - loss: 0.0194 - accuracy: 0.9934 - val_loss: 0.0152 - val_accuracy: 0.9950\n",
      "Epoch 9/20\n",
      "422/422 - 121s - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9967\n",
      "Epoch 10/20\n",
      "422/422 - 125s - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0112 - val_accuracy: 0.9955\n",
      "Epoch 11/20\n",
      "422/422 - 122s - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0080 - val_accuracy: 0.9978\n",
      "Epoch 12/20\n",
      "422/422 - 121s - loss: 0.0102 - accuracy: 0.9965 - val_loss: 0.0095 - val_accuracy: 0.9972\n",
      "Epoch 13/20\n",
      "422/422 - 131s - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.0054 - val_accuracy: 0.9983\n",
      "Epoch 14/20\n",
      "422/422 - 125s - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.0057 - val_accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "422/422 - 119s - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
      "Epoch 16/20\n",
      "422/422 - 117s - loss: 0.0081 - accuracy: 0.9972 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
      "Epoch 17/20\n",
      "422/422 - 116s - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0074 - val_accuracy: 0.9973\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0347 - accuracy: 0.9910\n",
      "--- Starting trial: run-3\n",
      "{'filter_size': 5, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 - 121s - loss: 1.1121 - accuracy: 0.7115 - val_loss: 0.3545 - val_accuracy: 0.9022\n",
      "Epoch 2/20\n",
      "422/422 - 119s - loss: 0.3052 - accuracy: 0.9124 - val_loss: 0.2436 - val_accuracy: 0.9320\n",
      "Epoch 3/20\n",
      "422/422 - 121s - loss: 0.2305 - accuracy: 0.9325 - val_loss: 0.1951 - val_accuracy: 0.9413\n",
      "Epoch 4/20\n",
      "422/422 - 126s - loss: 0.1878 - accuracy: 0.9450 - val_loss: 0.1700 - val_accuracy: 0.9495\n",
      "Epoch 5/20\n",
      "422/422 - 122s - loss: 0.1612 - accuracy: 0.9532 - val_loss: 0.1362 - val_accuracy: 0.9573\n",
      "Epoch 6/20\n",
      "422/422 - 121s - loss: 0.1432 - accuracy: 0.9581 - val_loss: 0.1323 - val_accuracy: 0.9605\n",
      "Epoch 7/20\n",
      "422/422 - 123s - loss: 0.1283 - accuracy: 0.9626 - val_loss: 0.1189 - val_accuracy: 0.9645\n",
      "Epoch 8/20\n",
      "422/422 - 113s - loss: 0.1175 - accuracy: 0.9655 - val_loss: 0.1242 - val_accuracy: 0.9635\n",
      "Epoch 9/20\n",
      "422/422 - 118s - loss: 0.1092 - accuracy: 0.9679 - val_loss: 0.0999 - val_accuracy: 0.9720\n",
      "Epoch 10/20\n",
      "422/422 - 120s - loss: 0.1016 - accuracy: 0.9700 - val_loss: 0.0896 - val_accuracy: 0.9743\n",
      "Epoch 11/20\n",
      "422/422 - 121s - loss: 0.0952 - accuracy: 0.9719 - val_loss: 0.0966 - val_accuracy: 0.9722\n",
      "Epoch 12/20\n",
      "422/422 - 120s - loss: 0.0910 - accuracy: 0.9738 - val_loss: 0.0838 - val_accuracy: 0.9742\n",
      "Epoch 13/20\n",
      "422/422 - 121s - loss: 0.0862 - accuracy: 0.9745 - val_loss: 0.0805 - val_accuracy: 0.9745\n",
      "Epoch 14/20\n",
      "422/422 - 121s - loss: 0.0823 - accuracy: 0.9756 - val_loss: 0.0727 - val_accuracy: 0.9777\n",
      "Epoch 15/20\n",
      "422/422 - 120s - loss: 0.0795 - accuracy: 0.9772 - val_loss: 0.0780 - val_accuracy: 0.9763\n",
      "Epoch 16/20\n",
      "422/422 - 113s - loss: 0.0773 - accuracy: 0.9770 - val_loss: 0.0703 - val_accuracy: 0.9800\n",
      "Epoch 17/20\n",
      "422/422 - 117s - loss: 0.0729 - accuracy: 0.9787 - val_loss: 0.0747 - val_accuracy: 0.9787\n",
      "Epoch 18/20\n",
      "422/422 - 128s - loss: 0.0709 - accuracy: 0.9790 - val_loss: 0.0685 - val_accuracy: 0.9805\n",
      "Epoch 19/20\n",
      "422/422 - 112s - loss: 0.0682 - accuracy: 0.9802 - val_loss: 0.0591 - val_accuracy: 0.9817\n",
      "Epoch 20/20\n",
      "422/422 - 125s - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0687 - val_accuracy: 0.9793\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0579 - accuracy: 0.9832\n",
      "--- Starting trial: run-4\n",
      "{'filter_size': 7, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 - 104s - loss: 0.2319 - accuracy: 0.9335 - val_loss: 0.0825 - val_accuracy: 0.9750\n",
      "Epoch 2/20\n",
      "422/422 - 104s - loss: 0.0695 - accuracy: 0.9792 - val_loss: 0.0543 - val_accuracy: 0.9837\n",
      "Epoch 3/20\n",
      "422/422 - 104s - loss: 0.0492 - accuracy: 0.9854 - val_loss: 0.0348 - val_accuracy: 0.9903\n",
      "Epoch 4/20\n",
      "422/422 - 103s - loss: 0.0400 - accuracy: 0.9873 - val_loss: 0.0314 - val_accuracy: 0.9908\n",
      "Epoch 5/20\n",
      "422/422 - 106s - loss: 0.0320 - accuracy: 0.9899 - val_loss: 0.0312 - val_accuracy: 0.9903\n",
      "Epoch 6/20\n",
      "422/422 - 105s - loss: 0.0262 - accuracy: 0.9916 - val_loss: 0.0234 - val_accuracy: 0.9922\n",
      "Epoch 7/20\n",
      "422/422 - 91s - loss: 0.0223 - accuracy: 0.9935 - val_loss: 0.0194 - val_accuracy: 0.9940\n",
      "Epoch 8/20\n",
      "422/422 - 93s - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0146 - val_accuracy: 0.9957\n",
      "Epoch 9/20\n",
      "422/422 - 93s - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0104 - val_accuracy: 0.9970\n",
      "Epoch 10/20\n",
      "422/422 - 96s - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0126 - val_accuracy: 0.9965\n",
      "Epoch 11/20\n",
      "422/422 - 93s - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0101 - val_accuracy: 0.9968\n",
      "Epoch 12/20\n",
      "422/422 - 92s - loss: 0.0098 - accuracy: 0.9968 - val_loss: 0.0136 - val_accuracy: 0.9957\n",
      "Epoch 13/20\n",
      "422/422 - 93s - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.0030 - val_accuracy: 0.9993\n",
      "Epoch 14/20\n",
      "422/422 - 93s - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0071 - val_accuracy: 0.9978\n",
      "Epoch 15/20\n",
      "422/422 - 93s - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.0041 - val_accuracy: 0.9990\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0339 - accuracy: 0.9899\n",
      "--- Starting trial: run-5\n",
      "{'filter_size': 7, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 - 105s - loss: 1.0617 - accuracy: 0.7456 - val_loss: 0.4016 - val_accuracy: 0.8838\n",
      "Epoch 2/20\n",
      "422/422 - 103s - loss: 0.3396 - accuracy: 0.9022 - val_loss: 0.2679 - val_accuracy: 0.9257\n",
      "Epoch 3/20\n",
      "422/422 - 109s - loss: 0.2508 - accuracy: 0.9276 - val_loss: 0.2463 - val_accuracy: 0.9292\n",
      "Epoch 4/20\n",
      "422/422 - 100s - loss: 0.2051 - accuracy: 0.9409 - val_loss: 0.1907 - val_accuracy: 0.9448\n",
      "Epoch 5/20\n",
      "422/422 - 95s - loss: 0.1764 - accuracy: 0.9484 - val_loss: 0.1614 - val_accuracy: 0.9518\n",
      "Epoch 6/20\n",
      "422/422 - 102s - loss: 0.1547 - accuracy: 0.9543 - val_loss: 0.1494 - val_accuracy: 0.9575\n",
      "Epoch 7/20\n",
      "422/422 - 103s - loss: 0.1396 - accuracy: 0.9594 - val_loss: 0.1487 - val_accuracy: 0.9575\n",
      "Epoch 8/20\n",
      "422/422 - 110s - loss: 0.1277 - accuracy: 0.9624 - val_loss: 0.1135 - val_accuracy: 0.9683\n",
      "Epoch 9/20\n",
      "422/422 - 112s - loss: 0.1182 - accuracy: 0.9654 - val_loss: 0.1151 - val_accuracy: 0.9678\n",
      "Epoch 10/20\n",
      "422/422 - 101s - loss: 0.1118 - accuracy: 0.9670 - val_loss: 0.1022 - val_accuracy: 0.9700\n",
      "Epoch 11/20\n",
      "422/422 - 104s - loss: 0.1057 - accuracy: 0.9693 - val_loss: 0.0905 - val_accuracy: 0.9730\n",
      "Epoch 12/20\n",
      "422/422 - 104s - loss: 0.0991 - accuracy: 0.9706 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
      "Epoch 13/20\n",
      "422/422 - 101s - loss: 0.0946 - accuracy: 0.9722 - val_loss: 0.0911 - val_accuracy: 0.9732\n",
      "Epoch 14/20\n",
      "422/422 - 99s - loss: 0.0890 - accuracy: 0.9737 - val_loss: 0.0876 - val_accuracy: 0.9758\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.0898 - accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "# Performing a grid search on the hyperparameters needed for test\n",
    "session_num = 0\n",
    "\n",
    "for filter_size in HP_FILTER_SIZE.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "    \n",
    "        hparams = {\n",
    "            HP_FILTER_SIZE: filter_size,\n",
    "            HP_OPTIMIZER: optimizer\n",
    "        }\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run('logs/hparam_tuning/' + run_name, hparams)\n",
    "\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualizing in Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7816), started 22:21:28 ago. (Use '!kill 7816' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ec987a5c89e72b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ec987a5c89e72b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/hparam_tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8964), started 0:00:20 ago. (Use '!kill 8964' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1510158ddcc1e1a6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1510158ddcc1e1a6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Accuracy table, it can be seen that the Adam optimizers perform better.\n",
    "\n",
    "Also, it the number of filters should not be too high or too small, it should just be at a midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6006: logdir logs/hparam_tuning (started 3:38:41 ago; pid 11980)\n",
      "  - port 6006: logdir {logs_base_dir} (started 3:40:56 ago; pid 13704)\n",
      "  - port 6006: logdir logs/fit (started 1 day, 19:01:14 ago; pid 1608)\n",
      "  - port 6006: logdir logs/hparam_tuning (started 17:47:13 ago; pid 7816)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To rerun tensorboard after the first run if there are any problems, open cmd and use the following commands\n",
    "\n",
    ".../>taskkill /im tensorboard.exe /f\n",
    "\n",
    ".../>del /q %TMP%\\.tensorboard-info\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
